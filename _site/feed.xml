<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>Actually, less is more!</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 18 Apr 2017 21:39:44 +0800</pubDate>
    <lastBuildDate>Tue, 18 Apr 2017 21:39:44 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>svm-支持向量机</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#目标函数推导&quot; id=&quot;markdown-toc-目标函数推导&quot;&gt;目标函数推导&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#拉格朗日法&quot; id=&quot;markdown-toc-拉格朗日法&quot;&gt;拉格朗日法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#smo&quot; id=&quot;markdown-toc-smo&quot;&gt;smo&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#求解二次规划的解析法&quot; id=&quot;markdown-toc-求解二次规划的解析法&quot;&gt;求解二次规划的解析法&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#选择alpha的启发式算法&quot; id=&quot;markdown-toc-选择alpha的启发式算法&quot;&gt;选择$\alpha$的启发式算法&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#总结&quot; id=&quot;markdown-toc-总结&quot;&gt;总结&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;目标函数推导&quot;&gt;目标函数推导&lt;/h2&gt;
&lt;p&gt;点到超平面的距离公式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {|\omega^Tx+b|} {|| \omega ||}&lt;/script&gt;

&lt;p&gt;在分类过程中，$ f(x) = \omega^Tx+b $ 预测出的符号应与类标记$y$一致。故$y(\omega^Tx+b)$的正负性可以表示分类的正确性。
函数间隔：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Upsilon=y {f(x)}&lt;/script&gt;

&lt;p&gt;几何间隔：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Upsilon=y \frac {f(x)} {||\omega||}&lt;/script&gt;

&lt;p&gt;超平面最小几何间隔：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\overline{\Upsilon}}=\min y \frac {f(x)} {||\omega||}&lt;/script&gt;

&lt;p&gt;最大间隔分类器的目标函数就是最大化最小几何间隔${\overline{\Upsilon}}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max {\overline{\Upsilon}}=\max \min y \frac {f(x)} {||\omega||}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170417220540043?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;即：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max {\overline{\Upsilon}}=\max y \frac {f(x)} {||\omega||}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;st. y_i(\omega_Tx_i+b) \geq \Upsilon&lt;/script&gt;

&lt;p&gt;令函数间隔为1，则$y {f(x)}=1$,则超平面最小几何间隔为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\overline{\Upsilon}}=\min  \frac 1 {||\omega||}&lt;/script&gt;

&lt;p&gt;从而目标函数转为:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max \frac 1 {||\omega||}=\min \frac 1 2 ||\omega||^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;st. y_i(\omega_Tx_i+b) \geq 1&lt;/script&gt;

&lt;p&gt;现在目标函数是二次的，约束条件是线性的，所以这是一个凸二次规划问题。&lt;/p&gt;
&lt;h2 id=&quot;拉格朗日法&quot;&gt;拉格朗日法&lt;/h2&gt;
&lt;p&gt;使用拉格朗日法，得到对偶问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\omega,b,\alpha)=\frac 1 2 ||\omega||^2 - \sum_{i=1}^n \alpha_i(y_i(\omega^T x_i+b)-1&lt;/script&gt;

&lt;p&gt;首先对$\omega $和b求最小化，得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega^* = \sum_{i=1}^n \alpha_i y_i x_i&lt;/script&gt;

&lt;p&gt;若存在$\alpha_i \geq 0$,则对应的就是在分界线上的支持向量，那么$y_i(\omega^&lt;em&gt;.x_i+b^&lt;/em&gt;)-1=0$.则等式两边同时乘$y_i$：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b^*=y_i- \sum_{i=1}^n \alpha_i y_i (x_i.x_j)&lt;/script&gt;

&lt;p&gt;根据&lt;script type=&quot;math/tex&quot;&gt;f(x) = \omega^{*T}x+b&lt;/script&gt;可得分类结果
将$\omega^* 和b^*$ 带回到目标函数后，可以得到关于$\alpha$的函数，通过SMO等方法可以解出。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_\alpha \sum_{i=1}^n  \alpha_i - \frac 1 2 \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^Tx_j&lt;/script&gt;

&lt;p&gt;s.t.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \leq \alpha_i \leq C&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ \sum_{i=1}^n  \alpha_i y_i=0&lt;/script&gt;

&lt;h2 id=&quot;smo&quot;&gt;smo&lt;/h2&gt;
&lt;h3 id=&quot;求解二次规划的解析法&quot;&gt;求解二次规划的解析法&lt;/h3&gt;
&lt;p&gt;在 αi = {α1, α2, α3, …, αn} 上求上述目标函数的最小值。为 了求解这些乘子,每次从中任意抽取两个乘子 α1 和 α2,然后固定 α1 和 α2 以外的其 它乘子 {α3, …, αn},使得目标函数只是关于 α1 和 α2 的函数。这样,不断的从一堆乘 子中任意抽取两个求解,不断的迭代求解子问题,最终达到求解原问题的目的。
假设选择的是两个变量α1, α2，其他变量{α3, …, αn}是固定的，那么子问题可以写成：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\alpha_1,\alpha_2} W(\alpha_1,\alpha_2)=\frac 1 2 K_{11} \alpha_1^2 +\frac 1 2 K_{22} \alpha_2^2+y_1 y_2 K_{12} \alpha_1 \alpha_2 - (\alpha_1+\alpha_2)+y_1 \alpha_1\sum_{i=3}^N y_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^N y_i\alpha_iK_{i2}&lt;/script&gt;

&lt;p&gt;s.t.
&lt;script type=&quot;math/tex&quot;&gt;{0 \leq \alpha_i \leq C}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_1 y_1+ \alpha_2 y_2=- \sum_{i=3}^N  \alpha_i y_i=\epsilon&lt;/script&gt;

&lt;p&gt;其中,$\epsilon$是常数。两个因子不好同时求解,所以可先求第二个乘子 $\alpha_2$ 的解($\alpha_2^{new}$),
得到$\alpha_2$的解($\alpha_2^{new}$)之后,再用$\alpha_2$ 的解($\alpha_2^{new}$)表示$\alpha_1$解($\alpha_2^{new}$),。为了求解$\alpha_1^{new}$,
得先确定 ($\alpha_2^{new}$), 的取值范围。假设它的上下边界分别为 H 和 L,那么有:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{L \leq \alpha_2^{new} \leq H}&lt;/script&gt;

&lt;p&gt;y1和y2不同时，当做+-1
y1和y2相同时，都当做1
则可以画出两个取值图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170417212050669?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;设预测值为：&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;g(x)=\sum_{i=1}^N\alpha_i y_i K(x_i,x)+b&lt;/script&gt;
则对输入预测值与真实输出之差&lt;/p&gt;

&lt;p&gt;得到关于单变量$\alpha_2$的未考虑不等式约束$ {0 \leq \alpha_i \leq C} $时的解：
&lt;img src=&quot;http://img.blog.csdn.net/20170417213927967?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;选择alpha的启发式算法&quot;&gt;选择$\alpha$的启发式算法&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;外层循环寻找第一个乘子$\alpha_1$，找到不满足kkt的&lt;/li&gt;
  &lt;li&gt;内层循环寻找满足条件：$\max |E_i-E_j|$的乘子
&lt;img src=&quot;http://img.blog.csdn.net/20170417214747986?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;p&gt;smo是一种启发式算法，基本思路是：如果所有变量都满足kkt条件，那么最优化问题的解就得到了。否则选择2个变量，固定其余变量，针对这2个变量构建一个二次规划子问题，这时可以通过解析法求解。如此可以通过不断的将原问题拆解为子问题达到求解原问题的目的。&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Apr 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/04/11/smo/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/11/smo/</guid>
        
        <category>svm</category>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>次梯度</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#次导数&quot; id=&quot;markdown-toc-次导数&quot;&gt;次导数&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#定义&quot; id=&quot;markdown-toc-定义&quot;&gt;定义&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#例子&quot; id=&quot;markdown-toc-例子&quot;&gt;例子&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#性质&quot; id=&quot;markdown-toc-性质&quot;&gt;性质&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#次梯度法&quot; id=&quot;markdown-toc-次梯度法&quot;&gt;次梯度法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;次导数&quot;&gt;次导数&lt;/h2&gt;
&lt;p&gt;设f在实数域上是一个凸函数，定义在数轴上的开区间内。这种函数不一定是处处可导的，例如绝对值函数$f(x) = |x|$ 。对于下图来说，对于定义域中的任何x0，我们总可以作出一条直线，它通过点(x0, f(x0))，并且要么接触f的图像，要么在它的下方。直线的斜率称为函数的次导数。次导数的集合称为函数f在x0处的次微分。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/4/4e/Subderivative_illustration.png&quot; alt=&quot;不可微函数&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;对于所有x，我们可以证明在点$x_0$ 的次导数的集合是一个非空闭区间[a,b]，其中a和b是单测极限。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = \lim_{x-&gt;x^-_0} \frac {f(x)- f(x_0)}{x-x_0}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b = \lim_{x-&gt;x^+_0} \frac {f(x)- f(x_0)}{x-x_0}&lt;/script&gt;

&lt;p&gt;一定存在，且a&amp;lt;=b，在[a,b]内的所有次导数是f在x0的次微分。&lt;/p&gt;
&lt;h3 id=&quot;例子&quot;&gt;例子&lt;/h3&gt;
&lt;p&gt;凸函数$f(x)=|x|$。在原点的次微分是[-1,1]。当x0&amp;lt;0时，次微分是单元素集合{-1},而x0&amp;gt;0时，次微分单元素集合是{1}。&lt;/p&gt;
&lt;h3 id=&quot;性质&quot;&gt;性质&lt;/h3&gt;
&lt;p&gt;当函数在x0处可导时，次微分只有一个点组成，这个点就是函数在x0处的导数。&lt;/p&gt;
&lt;h2 id=&quot;次梯度法&quot;&gt;次梯度法&lt;/h2&gt;
&lt;p&gt;次梯度方法(subgradient method)是传统的梯度下降方法的拓展，用来处理不可导的凸函数。它的优势是比传统方法处理问题范围大，劣势是算法收敛速度慢。但它对不可导函数有很好的处理方法。
通过求函数在点的每一分量的次导数可以求出函数在该点的次梯度。&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Apr 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/04/11/sub-gradient/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/11/sub-gradient/</guid>
        
        <category>svm</category>
        
        
        <category>math</category>
        
      </item>
    
      <item>
        <title>svm随机次梯度下降算法-pegasos</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#基本思想&quot; id=&quot;markdown-toc-基本思想&quot;&gt;基本思想&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#摘要&quot; id=&quot;markdown-toc-摘要&quot;&gt;摘要&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#引言&quot; id=&quot;markdown-toc-引言&quot;&gt;引言&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pegasos-算法&quot; id=&quot;markdown-toc-pegasos-算法&quot;&gt;pegasos 算法&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#基本算法&quot; id=&quot;markdown-toc-基本算法&quot;&gt;基本算法&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#伪代码如下&quot; id=&quot;markdown-toc-伪代码如下&quot;&gt;伪代码如下：&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#稀疏特征&quot; id=&quot;markdown-toc-稀疏特征&quot;&gt;稀疏特征&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#核函数解决非线性分类问题&quot; id=&quot;markdown-toc-核函数解决非线性分类问题&quot;&gt;核函数解决非线性分类问题&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#结合偏置项b&quot; id=&quot;markdown-toc-结合偏置项b&quot;&gt;结合偏置项b&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#实验比对&quot; id=&quot;markdown-toc-实验比对&quot;&gt;实验比对&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#线性核&quot; id=&quot;markdown-toc-线性核&quot;&gt;线性核&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#高斯核非线性核&quot; id=&quot;markdown-toc-高斯核非线性核&quot;&gt;高斯核（非线性核）&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;基本思想&quot;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;使用随机梯度下降直接解SVM的原始问题。&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文研究和分析了基于随机梯度下降的SVM优化算法，简单且高效。（Ο是渐进上界，Ω是渐进下界）本文证明为获得一定准确率精度$ \epsilon$所需的迭代次数满足$O{(\frac1 \epsilon)}$，且每一次迭代都只使用一个训练样本。相比之下，以前分析的SVM随机梯度下降次数满足$\Omega{(\frac {1} {\epsilon^2})}$。以前设计的SVM中，迭代次数也与$\frac 1 \lambda$线性相关 。对于线性核，pegasos算法的总运行时间是$O{(\frac d {\epsilon \lambda})}$,其中d是每一个样本中非零特征在边界上的个数。因此，运行时间和训练集的大小不是直接相关的，那么本算法尤其适合大规模的数据集。本文算法也可以拓展到非线性核上，但只能基于原始的目标函数，此时的运行时间和数据集大小线性相关$O{(\frac m{\epsilon \lambda})}$。本文算法尤其适用于大规模文本分类问题，比之前的SVM加速了几个数量级。&lt;/p&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;SVM是一种高效流行的分类器、SVM是一种典型的受限二次规划问题。然而，基于基本形式，SVM实际上是一个附带惩罚因子的无限制经验损失最小化问题。
定义目标函数为
 &lt;script type=&quot;math/tex&quot;&gt;f(\omega) = \min_w \frac{\lambda}{2}||\omega||^2 + \frac 1 m \sum l(\omega,(x,y))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
l(\omega,(x,y))=max\{0,1-y&lt;\omega,x&gt;\} %]]&gt;&lt;/script&gt;

&lt;p&gt;使用随机梯度下降解目标函数的算法称作pegasos。在每一次的迭代中，随机挑选一个训练样本计算目标函数的梯度，然后在在相反的方向走一个预订好的步长。根据上述算法，整体算法的运行时间满足$O{(\frac n {\lambda \epsilon})}$,其中n是w和x的维度。算法可以进一步优化为$O{(\frac d {\epsilon \lambda})}$,其中d是每一个样本中非零特征在边界上的个数。pegasos只与算法的随机步骤有关，和数据集无关。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Interior Point (IP) methods：可以使用拟牛顿法对目标函数进行优化，但算法因为使用了如海森矩阵等便需$m^2$的空间复杂度， 其中m是样本数。&lt;/li&gt;
  &lt;li&gt;Decomposition methods: 使用类似SMO的算法解SVM的对偶优化问题，但是这会导致相对比较慢的收敛速率。&lt;/li&gt;
  &lt;li&gt;Primal optimization:SVM大部分的求解方法都在解决对偶问题，尤其是在非线性核问题上。因为，即使SVM使用了非线性核，我们也可以将$\omega$替换为 $\sum \alpha_i y_i x_i$，从而将解决原目标函数的受约束优化问题转化为没有约束条件的优化问题。其实，直接解决原问题也有人研究过，比如使用smooth loss functions代替 hinge loss，这样将问题变成一个平滑的不受限制的优化问题，然后使用梯度下降和牛顿法来求解。本文采取了类似方法，针对hinge loss的不可微性，直接使用sub-gradients（子梯度）代替梯度。&lt;/li&gt;
  &lt;li&gt;尽管随机梯度下降算法是最差的优化算法，但是可以产生最好的泛化性能。&lt;/li&gt;
  &lt;li&gt;有一种基于随机子梯度下降的算法和pegasos比较类似，但是其非常重要的一部分是学习步长的设置，收敛速率最坏的上界是$\Omega{(\frac {1} {(\lambda \epsilon)^2})}$。欧几里得距离的平方可以被优化到0，但是收敛速率决定于步长参数的设置。pegasos调参简单。&lt;/li&gt;
  &lt;li&gt;online methods：在线学习算法一般与随机梯度下降算法结合，如每一次迭代只使用一个样本。在线学习算法被提出为SVM的快速替代方案。在线算法可以使用在线到批量转换方案来获得具有低泛化误差的预测器，但是这种方法不一定能够解决原问题的$\epsilon$精度要求，并且它们的性能通常低于直接批量优化器。如上所述，Pegasos共享了在线学习算法的简单性和速度，但它保证融合到最优的SVM解决方案。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;pegasos-算法&quot;&gt;pegasos 算法&lt;/h2&gt;
&lt;p&gt;基于上述提到的方法，pegasos在原始目标函数上通过精致挑选的步长使用随机梯度下降求解。将在本部分详细描述pegasos的过程并提供伪代码。同时也阐述基本算法的一系列变种，并讨论几个实际问题。&lt;/p&gt;
&lt;h3 id=&quot;基本算法&quot;&gt;基本算法&lt;/h3&gt;
&lt;p&gt;随机选取一个训练样本$it$ 其中i代表选取的样本，t代表迭代的次数，带入&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\omega) = \min_w \frac{\lambda}{2}||\omega||^2 + \frac 1 m \sum l(\omega,(x,y))&lt;/script&gt;

&lt;p&gt;得到近似的目标函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\omega,i_t) = \frac{\lambda}{2}||\omega||^2 +  l(\omega,(x_{it},y_{it}))&lt;/script&gt;

&lt;p&gt;计算次梯度得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\nabla_t = \lambda \omega_t - I[y_{it} \{\omega_t,x_{it}\}&lt;1] y_{it}x_{it} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中，$I[y_{it} {\omega_t,x_{it}}&amp;lt;1]$ 是指示函数，如果是真则值为1，反之为0 。&lt;/p&gt;
&lt;h4 id=&quot;伪代码如下&quot;&gt;伪代码如下：&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170411144957908?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;输入数据集S,正则化因子$\lambda$，迭代次数T,则每一次迭代式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\omega_t+1  &lt;= {\omega_t}-\eta_t\nabla_t %]]&gt;&lt;/script&gt;

&lt;p&gt;$\eta_t = \frac 1 {\lambda t}$是变动步长，和学习率$\lambda$成反比，随着迭代次数t的增加而减少。联合$\nabla_t = \lambda \omega_t - I[y_{it} {\omega_t,x_{it}}&amp;lt;1] y_{it}x_{it}$展开迭代式得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\omega_{t+1}  &lt;= {\omega_t}-\eta_t [\lambda \omega_t - I[y_{it} \{\omega_t,x_{it}\}&lt;1] y_{it}x_{it} ] %]]&gt;&lt;/script&gt;

&lt;p&gt;进一步推倒得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\omega_{t+1 } &lt;= {(1-\frac1 t)\omega_t}+\eta_t  I[y_{it} \{\omega_t,x_{it}\}&lt;1] y_{it}x_{it} ] %]]&gt;&lt;/script&gt;

&lt;p&gt;如果指示函数为真，则：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\omega_t+1  &lt;= {(1-\frac1 t)\omega_t}+\eta_t   y_{it}x_{it} %]]&gt;&lt;/script&gt;

&lt;p&gt;如果为假，则&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\omega_t+1  &lt;= {(1-\frac1 t)\omega_t} %]]&gt;&lt;/script&gt;

&lt;p&gt;迭代T后，输出$\omega_t+1$&lt;/p&gt;

&lt;p&gt;类似的mini-batch  pegasos算法伪代码如下：
&lt;img src=&quot;http://img.blog.csdn.net/20170413133110678?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;稀疏特征&quot;&gt;稀疏特征&lt;/h3&gt;
&lt;p&gt;当训练集样本是稀疏的时候，可以把$\omega$表示成向量v 和标量a相乘。$\omega=a v$。那么容易证明每一次迭代的计算运行次数O(d)，d是非零元素的个数。&lt;/p&gt;
&lt;h2 id=&quot;核函数解决非线性分类问题&quot;&gt;核函数解决非线性分类问题&lt;/h2&gt;
&lt;p&gt;SVM的主要优势是可以使用核方法直接在特征空间分类。可以如此做关键是表示定理（ Representer Theorem ），根据表示定理可以通过训练数据的线性组合求得原目标函数的最优解。即在训练SVM的时候可以不直接使用训练数据，通过指定核操作的训练数据的内积结果训练。也就是说，不考虑训练样本x本身的线性函数的预测变量，我们考虑训练样本的隐式映射φ（x）后的线性函数的预测变量。
推导过程如下：
如求解线性不可分的$f(\omega)$,将x映射到特征空间：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\omega) = \min_w \frac{\lambda}{2}||\omega||^2 + \frac 1 m \sum_{(x,y) \in S} l(\omega;(\phi(x),y))&lt;/script&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
l(\omega;(\phi(x),y))=max\{0,1-y&lt;\omega,\phi(x)&gt;\} %]]&gt;&lt;/script&gt;

&lt;p&gt;为使用核函数将内积$&amp;lt;\omega,\phi(x)&amp;gt;$映射到特征空间的内积，将$\omega$用x来表示，通过对原目标函数求次梯度可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\omega_{t+1 } = {(1-\frac1 t)\omega_t}+\eta_t  I[y_{it} \{\omega_t,\phi(x_{it})\}&lt;1] y_{it}\phi(x_{it}) %]]&gt;&lt;/script&gt;

&lt;p&gt;设：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
v_t =  I[y_{it} \{\omega_t,\phi(x_{it})\}&lt;1] y_{it}\phi(x_{it}) %]]&gt;&lt;/script&gt;

&lt;p&gt;则迭代式重写为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{t+1 } = {(1-\frac1 t)\omega_t}+\eta_t v_t&lt;/script&gt;

&lt;p&gt;根据迭代式的递推可得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{t+1 } = {(\frac {t-1} t)\omega_t}+\eta_t v_t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{t+1 } = \frac 1 {\lambda t} \sum_{i=1}^t v_t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\omega_{t+1} = \frac 1 {\lambda t} \sum_{i=1}^t I[y_{it} \{\omega_t,\phi(x_{it})\}&lt;1] y_{it}\phi(x_{it}) %]]&gt;&lt;/script&gt;

&lt;p&gt;设$\alpha_{t+1}[j]$ 为第t+1次，样本j被选中且损失不为0的次数（指示函数为1）。
则：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_{t+1} = \frac 1 {\lambda t} \sum_{j=1}^m \alpha_{t+1}[j] y_{it}\phi(x_{it})&lt;/script&gt;

&lt;p&gt;带回到$l(\omega)$，通过T次随机次梯度下降伪代码如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170413112616958?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;根据输出$\alpha_{T+1}$得到$\omega$
映射φ（·）并不明确指定，而是训练样本通过映射φ（·）之后通过核计算K（x，x’）=φ（x），φ（x’）得到内积结果。本文中，无需将目标函数转换为对偶函数，直接最小化原始问题，同时仍然使用核方法。Pegasos算法可以仅使用内核来实现求解，而不需要直接访问特征向量$\phi （x）$或显式访问权重向量$\omega$&lt;br /&gt;
因为需要遍历m个样本，故时间复杂度为$O{(\frac m {\epsilon \lambda})}$&lt;/p&gt;

&lt;h2 id=&quot;结合偏置项b&quot;&gt;结合偏置项b&lt;/h2&gt;
&lt;p&gt;当正负样本比例不均衡的时候，偏置项比较重要。
在算法的外部加一层找寻b的外循环，即对于不同的b值，均使用pegasos求解出各自的$\omega$。优化问题是$ \min_\omega \frac \lambda 2||w||^2+J(b;S)$目标函数如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(b;S) = \min_\omega \frac 1 m \sum l((\omega,b);(x,y))&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
J(b;S) = \min_\omega \frac 1 m \sum_{(x,y)\in S} [1-y(&lt;\omega,x&gt;+b)]_+ %]]&gt;&lt;/script&gt;
因为b是固定值，在内层循环求解时，仍然可以使用pegasos算法,且这种方法的时间复杂度仍然保持在$O{(\frac d {\epsilon \lambda})}$&lt;/p&gt;
&lt;h2 id=&quot;实验比对&quot;&gt;实验比对&lt;/h2&gt;
&lt;p&gt;pegasos算法与SDCA,SVM_LIGHT,LASVM比较&lt;/p&gt;
&lt;h3 id=&quot;线性核&quot;&gt;线性核&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170413120005507?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;
第一行是原目标函数的收敛曲线，第二行是分类误差率。横坐标是运行时间。&lt;/p&gt;

&lt;h3 id=&quot;高斯核非线性核&quot;&gt;高斯核（非线性核）&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170413120248762?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;这里写图片描述&quot; /&gt;
第一行是原目标函数的收敛曲线，第二行是log归约后的目标函数收敛曲线，第三行是分类误差率。横坐标是运行时间。&lt;/p&gt;

</description>
        <pubDate>Mon, 10 Apr 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/04/10/pegasos/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/10/pegasos/</guid>
        
        <category>svm</category>
        
        
        <category>machine learning</category>
        
      </item>
    
  </channel>
</rss>
