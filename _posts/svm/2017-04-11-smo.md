---
layout: post
title:  "svm-支持向量机"
date:   2017-04-11 
categories:
- machine learning
tags:
- svm
---

* content
{:toc}

## 目标函数推导
点到超平面的距离公式：

$$\frac {|\omega^Tx+b|} {|| \omega ||}$$

在分类过程中，$ f(x) = \omega^Tx+b $ 预测出的符号应与类标记$y$一致。故$y(\omega^Tx+b)$的正负性可以表示分类的正确性。
函数间隔：

$$\Upsilon=y {f(x)} $$

几何间隔：

$$\Upsilon=y \frac {f(x)} {||\omega||}$$

超平面最小几何间隔：

$${\overline{\Upsilon}}=\min y \frac {f(x)} {||\omega||}$$

最大间隔分类器的目标函数就是最大化最小几何间隔${\overline{\Upsilon}}$

$$\max {\overline{\Upsilon}}=\max \min y \frac {f(x)} {||\omega||}$$

![这里写图片描述](http://img.blog.csdn.net/20170417220540043?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

即：

$$\max {\overline{\Upsilon}}=\max y \frac {f(x)} {||\omega||}$$

$$st. y_i(\omega_Tx_i+b) \geq \Upsilon$$

令函数间隔为1，则$y {f(x)}=1$,则超平面最小几何间隔为

$${\overline{\Upsilon}}=\min  \frac 1 {||\omega||}$$

从而目标函数转为:

$$\max \frac 1 {||\omega||}=\min \frac 1 2 ||\omega||^2$$

$$st. y_i(\omega_Tx_i+b) \geq 1$$

现在目标函数是二次的，约束条件是线性的，所以这是一个凸二次规划问题。
## 拉格朗日法
使用拉格朗日法，得到对偶问题：

$$L(\omega,b,\alpha)=\frac 1 2 ||\omega||^2 - \sum_{i=1}^n \alpha_i(y_i(\omega^T x_i+b)-1$$

首先对$\omega $和b求最小化，得到：

$$\omega^* = \sum_{i=1}^n \alpha_i y_i x_i$$

若存在$\alpha_i \geq 0$,则对应的就是在分界线上的支持向量，那么$y_i(\omega^*.x_i+b^*)-1=0$.则等式两边同时乘$y_i$：

$$b^*=y_i- \sum_{i=1}^n \alpha_i y_i (x_i.x_j)$$

根据$$f(x) = \omega^{*T}x+b$$可得分类结果
将$\omega^* 和b^*$ 带回到目标函数后，可以得到关于$\alpha$的函数，通过SMO等方法可以解出。

$$\max_\alpha \sum_{i=1}^n  \alpha_i - \frac 1 2 \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^Tx_j$$

s.t. 

$$ 0 \leq \alpha_i \leq C$$

$$\ \sum_{i=1}^n  \alpha_i y_i=0$$

## smo
### 求解二次规划的解析法
在 αi = {α1, α2, α3, ..., αn} 上求上述目标函数的最小值。为 了求解这些乘子,每次从中任意抽取两个乘子 α1 和 α2,然后固定 α1 和 α2 以外的其 它乘子 {α3, ..., αn},使得目标函数只是关于 α1 和 α2 的函数。这样,不断的从一堆乘 子中任意抽取两个求解,不断的迭代求解子问题,最终达到求解原问题的目的。
假设选择的是两个变量α1, α2，其他变量{α3, ..., αn}是固定的，那么子问题可以写成：

$$ \min_{\alpha_1,\alpha_2} W(\alpha_1,\alpha_2)=\frac 1 2 K_{11} \alpha_1^2 +\frac 1 2 K_{22} \alpha_2^2+y_1 y_2 K_{12} \alpha_1 \alpha_2 - (\alpha_1+\alpha_2)+y_1 \alpha_1\sum_{i=3}^N y_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^N y_i\alpha_iK_{i2}
$$

s.t.
$$ {0 \leq \alpha_i \leq C} $$ 

$$ \alpha_1 y_1+ \alpha_2 y_2=- \sum_{i=3}^N  \alpha_i y_i=\epsilon$$

其中,$\epsilon$是常数。两个因子不好同时求解,所以可先求第二个乘子 $\alpha_2$ 的解($\alpha_2^{new}$),
得到$\alpha_2$的解($\alpha_2^{new}$)之后,再用$\alpha_2$ 的解($\alpha_2^{new}$)表示$\alpha_1$解($\alpha_2^{new}$),。为了求解$\alpha_1^{new}$,
得先确定 ($\alpha_2^{new}$), 的取值范围。假设它的上下边界分别为 H 和 L,那么有:

$$ {L \leq \alpha_2^{new} \leq H}		     $$ 

y1和y2不同时，当做+-1
y1和y2相同时，都当做1
则可以画出两个取值图：

![这里写图片描述](http://img.blog.csdn.net/20170417212050669?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

设预测值为：

$$g(x)=\sum_{i=1}^N\alpha_i y_i K(x_i,x)+b$$
则对输入预测值与真实输出之差

得到关于单变量$\alpha_2$的未考虑不等式约束$ {0 \leq \alpha_i \leq C} $时的解：
![这里写图片描述](http://img.blog.csdn.net/20170417213927967?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 选择$\alpha$的启发式算法
1. 外层循环寻找第一个乘子$\alpha_1$，找到不满足kkt的
2. 内层循环寻找满足条件：$\max |E_i-E_j|$的乘子
![这里写图片描述](http://img.blog.csdn.net/20170417214747986?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjc2MTI2Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 总结
smo是一种启发式算法，基本思路是：如果所有变量都满足kkt条件，那么最优化问题的解就得到了。否则选择2个变量，固定其余变量，针对这2个变量构建一个二次规划子问题，这时可以通过解析法求解。如此可以通过不断的将原问题拆解为子问题达到求解原问题的目的。

